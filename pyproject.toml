[tool.poetry]
name = "data-engineer-toolkit"
version = "0.1.0"
description = "Projeto exemplo spark usando o ambiente do docker"
authors = ["Rafael Negr√£o <rafael.negrao@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [
    { include = "orquestracao", from = "src" },
    { include = "spark", from = "src" }
]

[tool.pytest.init_options]
testpaths = ["tests"]
pythonpath = ["src"]

[tool.poetry.dev-dependencies]
localstack-client = "2.3"

[tool.poetry.dependencies]
python = "^3.8"
boto3 = "^1.23.5"
kafka-python = "2.0.2"
retry = "0.9.2"
pydash = "7.0.6"
pyspark = "3.3.0"
delta-spark = "2.1.0"
apache-airflow = { extras = ["apache-spark"], version = "^2.7.1" }
# apache-airflow = { extras = ["apache-spark"], version = "^2.7.1", optional = true }
# apache-airflow-providers-apache-spark = { version = "4.1.5", optional = true }

[tool.poetry.extras]
# airflow = ["apache-airflow"]
# airflow-providers = ["apache-airflow-providers-apache-spark"]

[tool.poetry.group.dev.dependencies]
pytest-cov = "^4.0.0"
pytest = "^7.2.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"